---
title: |
  | Understanding Correlates of Obesity:
  | Supervised and Unsupervised Learning Approaches
author: "Katherine M. Prioli"
date: "December 05, 2019"
output:
  pdf_document: default
  html_document: default
geometry: margin = 0.5in
abstract: "**_Background_** Increasing overweight and obesity represent an important national health concern and a significant, preventable driver of healthcare expenditure.  Every two years, the National Health and Nutrition Examination Survey (NHANES) collects data about Americans' dietary and exercise behaviors along with demographics and relevant clinical characteristics.  If these characteristics can be used to identify overweight and obesity at a population level, interventions designed to reduce excess adiposity could be appropriately targeted to those who could benefit most.  This study applied supervised and unsupervised machine learning techniques to categorize NHANES cases by Body Mass Index (BMI) weight category.  **_Methods_** Medical, demographic, and behavioral variables known or suspected to be correlated with obesity were selected from the 2015-2016 NHANES data.  The dataset was subset to adults, missing continuous data were imputed, and BMI weight category (four levels) was computed for each case.  Random forest classification (supervised approach) was performed using *n* Ã— *k*-fold crossvalidation on both the initial dataset and a smaller dataset with dichotomous BMI categories.  Hierarchical clustering (unsupervised approach) was performed on a 5% sample stratified by BMI category using both agglomerative (AGNES) and divisive (DIANA) methods, and percentages of cases in each BMI category were calculated for each cluster.  Mean homogeneity was compared for AGNES vs. DIANA on three analyses:  1) four-level BMI categories, 2) dichotomous BMI categories, 3) dichotomous BMI categories and increased number of clusters.  **_Results_** The analytic dataset contained 5406 cases and 24 variables.  Initial random forest analysis using *k* = 5 folds, *n* = 5 repeats, and 100 trees yielded 52.6% mean accuracy (range 50.3-56.2%).  After hyperparameter tuning and variable reduction, the final random forest analysis yielded 75.6% mean accuracy (range 74.4-77.4%) on *k* = 3, *n* = 10, and 200 trees.  Clustering via DIANA outperformed AGNES for all three analyses, reaching 81.2% mean homogeneity (range 57.1-100.0%) in Analysis 3 vs. 77.5% (range 52.6-100.0%) for AGNES.  **_Conclusion_** Though refinements are needed to improve results, findings suggest that machine learning approaches can be leveraged to identify overweight and obese individuals."
---

```{r load_libraries, include = FALSE, ech = FALSE, message = FALSE, warning = FALSE, paged.print = FALSE}
library(here)           # For easy sourcing from project directory
library(haven)          # For loading SAS .xpt files
library(sjlabelled)     # For removing pesky column labels
library(forcats)        # For handling categorical data
library(psych)          # For describe()
library(randomForest)   # For rfImpute()
library(vegan)          # For vegdist() (computing Gower distance)
library(cluster)        # For diana()
library(fpc)            # For cqcluster.stats()
library(ggdendro)       # For ggdendrogram()
library(dendextend)     # For coloring dendrogram labels by BMI_cat
library(gridExtra)      # For grid.arrange()
library(grid)           # For textGrob() to annotate grid.arrange() elements
library(rmarkdown)      # For render()
library(reticulate)     # For interfacing with Python in .Rmd
library(kableExtra)     # For prettifying output tables
library(broom)          # For tidy()
library(ggthemr)        # For prettifying output plots
library(tidyselect)     # For selecting by string
library(tidyverse)      # For data import and wrangling
```

```{r incrementers, include = FALSE, message = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
i <- 1   # Tables
j <- 1   # Figures
```


## **Background**

Overweight and obesity are growing public health concerns, with 71.6% of American adults meeting at least the criteria for overweight, and 39.8% qualifying as obese per the Centers for Disease Control and Prevention (CDC) as of 2016.[1]  Defined as a Body Mass Index (BMI) of 30 $kg/m^{2}$ or above, obesity is of especially high concern because it strongly correlates with many diseases and conditions that lead to significant morbidity and mortality, and thus to increased healthcare utilization and decreased productivity, both at high economic burden.[2-4]

The National Health and Nutrition Examination Survey (NHANES), a biennial health surveillance study performed by the CDC, collects data from a nationally representative sample of community-dwelling Americans pertaining to demographics, health, and nutritional and exercise behaviors through both self-report and physical examination.[5]  NHANES has uncovered some demographic correlates of obesity $-$ namely, that obesity is more prevalent among Hispanics and non-Hispanic blacks for both adults and children, that obesity is more prevalent among women across all racial groups, and that for all racial groups except blacks, education level appears inversely correlated with prevalence of obesity.[6,7]  However, though much progress has been made toward understanding the socioeconomic and behavioral drivers of obesity, much remains unknown about the interactions of these characteristics with medical correlates and how constellations thereof could be used to detect obesity at the population level.

Reducing both the incidence and prevalence of obesity will be critical in managing obesity-related healthcare expenditure, especially for publicly funded programs such as Medicare and Medicaid.  If constellations of obesity correlates can be identified, tailored health interventions that target these constellations to prevent or reduce obesity could be developed.  Using the 2015-2016 NHANES data, the objective of this study was to apply both supervised and unsupervised machine learning approaches to find patterns in a carefully selected set of characteristics which are known or suspected to be correlated with obesity.


## **Methods**

### *Variable Selection*

A literature search of PubMed for currently known or suspected demographic, behavioral, and medical correlates of excess adiposity was used to inform variable selection.  NHANES 2015-2016 variables chosen for the study are presented in Table `r i`, with the source denoted as "native" for variables native to the NHANES dataset or "derived" for variables calculated from native NHANES variables.  Derived variables of note included the scored and categorized Patient Health Questionnaire (PHQ-9), a validated nine-item depression inventory, as well as `BP_cat`, representing clinical categories of blood pressure ranging from hypotension through hypertensive crisis, assigned based on systolic and diastolic blood pressure thresholds.  Additionally, many of the native categorical variables were refactored to group nonresponses (e.g., refusals to respond, "Don't know" responses, and missing values) into a single category.

**Table `r i`.  Variables in initial dataset**

```{r init_vars, echo = FALSE, message = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}

initvars <- read_csv(here("/report/init_vars_final_rept.csv"), col_names = TRUE) %>% 
  select(-Type) %>% 
  kable(format = "markdown")
initvars

i <- i + 1
```

Because BMI categories differ for children and adults, the analysis was limited to ages $\ge$ 20, consistent with the NHANES definition of adult.[8]  Additionally, since `BMI_cat` represents the data labels, all rows with null `BMI_cat` were excluded.

Descriptive statistics and frequency tables were generated for continuous and categorical variables respectively to understand data contents and to assess the degree of missingness among the continuous data.  Missing values were imputed for continuous data.  To avoid introducing bias through imputation, two-sided Wilcoxon Rank-Sum tests were performed at the $\alpha = 0.05$ level for each variable on the pre- vs. post-imputation data to identify any statistically significant changes introduced by imputation.  Any variables having statistically different post-imputation data were removed from the dataset.


### *Supervised Approach*

Using algorithms available in Scikit-learn, a random forest of decision tree classifiers was run with $n \times k$-fold crossvalidation to classify cases into BMI category, and descriptive statistics were generated to assess model accuracy.  To improve performance, hyperparameters `n_repeats` (or *n*, number of times crossvalidation was performed) and `n_splits` (or *k*, number of folds) in `RepeatedKFold()` were tuned, along with hyperparameters `n_estimators` (number of decision trees in the forest), `min_samples_leaf` (minimum cases allowed per terminal node), and `max_depth` (maximum tree depth) in `RandomForestClassifier`.

To further improve accuracy, the labels representing BMI category were recast as dichotomous, grouping together the underweight and healthy weight categories versus the overweight and obese categories.  Additionally, the six comorbidity variables (pertaining to diabetes, coronary artery disease, myocardial infarction, thyroid disease, hypertension, and depression) were consolidated into one variable representing number of comorbidities, and variables which were suspected not to contribute much information to the model (due to low variation or large amount of missingness) were excluded.  This modified dataset was then run through $N \times k$-fold crossvalidation with hyperparameter tuning and model accuracy assessment as previously described.

### *Unsupervised Approach*

For the unsupervised component of this analysis, data was subset to variables with $\ge 90$% non-missing values, then all cases with missing values were dropped.  Since this approach involves visual analysis via dendrograms, the data was further subset to a random 5% sample within each BMI category to allow for sensible, interpretable dendrogram plots.  The analytic dataset thus comprised `r dim(nhanes_unsup_mat)[1]` cases having `r dim(nhanes_unsup_mat)[2]` variables.  To generate dendrograms for agglomerative clustering (AGNES) and divisive clustering (DIANA), a dissimilarity matrix was calculated based on Gower distance.[21]  The Gower distance was chosen because the variables in this dataset are of mixed type (*i.e.*, some continuous, some ordinal, and some nominal).  Because the data is likely noisy, AGNES relied on complete (*i.e.*, maximum distance) linkage between clusters.

Radial dendrograms for both the agglomerative and divisive approaches were plotted along with colored labels at each leaf to indicate the four BMI weight categories.  For both AGNES and DIANA, cluster size was assessed and within-cluster sums of squares and average silhouette width (a measure of within- *vs.* between-cluster dissimilarity) were calculated based on varying the number of clusters up to an upper limit of 15; these latter two metrics were used to generate scree and silhouette plots, which informed the initial cluster number for each approach.[22]

Three analyses were performed.  In Analysis #1, the radial dendrograms were replotted using the cluster numbers suggested by the scree and silhouette plots to inform cluster coloration, and the homogeneity of each cluster was assessed via both visual and numeric inspection.  Analysis #2 used similar methods but applied dichotomous BMI categories (defined as not overweight or obese *vs.* overweight or obese).  Analysis #3 also used dichotomous BMI category along with an increased number of clusters.  For each analysis, results of AGNES and DIANA were compared.

### *Technologies*

Data import, wrangling, and visualization were performed in R using an RMarkdown notebook and relying primarily on the `tidyverse` ecosystem.  The $n \times k$ crossvalidation random forest was implemented via Python code chunks in the notebook leveraging Scikit-Learn along with other common Python libraries as necessary (*e.g.*, `numpy` and `pandas`).[23-25]  For the agglomerative and divisive clustering, R packages `cluster`, `fpc`, `ggdendro`, and `dendextend` were used.[26-29]  A public GitHub repository was established for this study and contains all raw data files and code along with project documentation.[30]


## **Results**

The final analytic dataframe contained `r dim(nhanes)[1]` rows and `r dim(nhanes)[2]` columns.  Variables included in the dataset are presented in Table `r i`.

**Table `r i`.  Variables in final dataset**

```{r analytic_vars, echo = FALSE, message = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
varnames <- colnames(nhanes) %>% 
  as_tibble() %>% 
  rename(Variable = value) %>% 
  arrange(Variable)

vardescs <- c("Age",
              "BMI weight category",
              "Calories consumed on previous day",
              "Previous day's calorie consumption compared to usual",
              "Previous day's water intake",
              "Feels diet is healthy",
              "Level of education",
              "Family income category",
              "Has eaten fast food in past 12 months",
              "Has used nutrition information to select fast food",
              "Would use nutrition information to select fast food",
              "Gender",
              "Told by doctor to lose weight and/or exercise",
              "Marital status",
              "Daily minutes of moderate to vigorous activity",
              "Daily minutes spent sedentary",
              "Number of comorbidities",
              "Race",
              "Has eaten at a restaurant in past 12 months",
              "Has used nutrition information to select restaurant meal",
              "Would use nutrition information to select restaurant meal",
              "Unique identifier (omitted from analyses)",
              "Walking limitations",
              "Work limitations") %>% 
  as_tibble() %>% 
  rename(Description = value)

dfvars <- cbind(varnames, vardescs) %>% 
  as_tibble() %>% 
  kable(format = "markdown")
dfvars

i <- i + 1
```

### *Supervised Approach*

The initial model, based on the full dataset and using $k = 5$ folds, $n = 5$ repeats, and 100 trees in the ensemble without limitation on the number of cases per terminal node, yielded `r paste0(scores_init$pct_acc, "%")`  mean accuracy (range: `r scores_init$min_acc`-`r paste0(scores_init$max_acc, "%")`).  Variables included in the reduced dataset included age, gender, race, education level, marital status, family income category, number of comorbidities, number of daily minutes active and sedentary, having eaten fast food or at a restaurant in the past year, daily calories, daily water intake, and having been told by a doctor to lose weight and/or exercise, along with dichotomous BMI category.  The final model based on this reduced dataset used $k = 3$ folds, $n = 10$ repeats, and 200 trees, and was limited to $\ge 25$ cases per terminal node.  This model yielded `r paste0(scores_final$pct_acc, "%")` mean accuracy (range: `r scores_final$min_acc`-`r paste0(scores_final$max_acc, "%")`), representing a meaningful improvement.

### *Unsupervised Approach*

<!-- Do I want to put the correlation plots in here? -->

The scree and silhouette plots are presented in Figure `r paste0(j, "a and ", j,"b")`. The AGNES scree plot shows a relatively smooth curve without strong elbows; possible but faint elbows appear at 5 and 7 clusters.  For DIANA, two elbows are seen:  one at 5 clusters and one at 6.  Both the AGNES and DIANA silhouette plots show maximum average silhouette width at 2 clusters, which is unlikely to contain sufficient data to be meaningful.  AGNES shows a local maximum at 7 clusters, and DIANA at 6.  Considered together, scree and silhouette plots indicate that 7 clusters may be sufficient for AGNES, and 6 for DIANA.

**Figure `r i`.  Scree and Silhouette Plots**

```{r scree_silhou, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 8}
grid.arrange(screes, sils, nrow = 2)

j <- j + 1
```

### *Analysis #1*

The initial radial dendrogram for the agglomerative method is shown in Fig. `r j` with the corresponding numeric analysis of BMI category by cluster number presented in Table `r i`.  Cluster #1 showed the most homogeneity, with 83.3% of the 12 cases in the cluster obese.  Cluster #2 was slightly larger at 25 cases, and was 64.0% homogeneous for obesity.  The largest cluster was Cluster #7, which was the least homogeneous with 37.7% of its 69 cases overweight, 36.2% obese, and 24.6% having healthy weight.  Mean cluster homogeneity was 52.8% (range 37.7 - 83.3%).

**Figure `r j`.  Analysis #1:  Clustered Radial Dendrogram, AGNES (*k* = 7)**

```{r agnes_dend_clust_init, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 8.25, fig.height = 6.75}
ggthemr("flat", spacing = 0.1)
agnes_dend_init_out <- ggdraw() + draw_image(ggleg_svg, x = -0.025, scale = 1.15) + draw_plot(agnes_dend_clust_init_gg, x = -0.15)
agnes_dend_init_out
ggthemr("flat")

j <- j + 1
```

**Table `r i`.  Analysis #1:  BMI Categories by Cluster, AGNES (*k* = 7)**

```{r agnes_dend_clust_init_aggreg, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 6.75, fig.height = 6.75}
ag_init_clusts_kbl

i <- i + 1
```

For DIANA, the initial radial dendrogram is shown in Fig. `r j` with analysis of BMI category by cluster number in Table `r i`.  A pattern generally similar to that seen in the initial agglomerative case is seen, wherein smaller clusters had better homogeneity, and heterogeneity increased with cluster size.  Here, Cluster #6 was the most homogeneous, with 77.3% of its 22 cases obese, and Cluster #2 was the least homogeneous, with $\frac{1}{3}$ of its 69 cases at healthy weight, another $\frac{1}{3}$ obese, and 31.9% overweight.  Mean cluster homogeneity was 54.7% (range 33.3 - 77.3%).

**Figure `r j`.  Analysis #1:  Clustered Radial Dendrogram, DIANA (*k* = 6)**

```{r diana_dend_clust_init, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 8.25, fig.height = 6.75}
ggthemr("flat", spacing = 0.1)
di_dend_init_out <- ggdraw() + draw_image(ggleg_svg, x = -0.025, scale = 1.15) + draw_plot(diana_dend_clust_init_gg, x = -0.1)
di_dend_init_out
ggthemr("flat")

j <- j + 1
```

**Table `r i`.  Analysis #1:  BMI Categories by Cluster, DIANA (*k* = 6)**

```{r diana_dend_clust_init_aggreg, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 6.75, fig.height = 6.75}
di_init_clusts_kbl

i <- i + 1
```

### *Analysis #2*

After dichotomizing BMI category, both AGNES and DIANA showed gains in homogeneity.  AGNES (Fig. `r j` and Table `r i`) had improved homogeneity in all seven clusters, with homogeneity ranging from 52.8% to 100%.  Importantly, the two largest clusters were over 70% homogeneous.  DIANA (Fig. `r j + 1` and Table `r i + 1`) showed similar improvements, with three clusters over 90% homogeneous, and homogeneity values ranging from 57.1% to 94.9%.  Mean homogeneity for the agglomerative approach was 76.3%, compared to 76.6% for the divisive approach.  Though these values are comparable, DIANA was able to achive these results with one fewer cluster than AGNES.

**Figure `r j`.  Analysis #2:  Clustered Radial Dendrogram with Dichotomous BMI Categories, AGNES (*k* = 7)**

```{r agnes_dend_clust_dichot, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 8.25, fig.height = 6.75}
ggthemr("flat", spacing = 0.1)
ag_dend_dichot_out <- ggdraw() + draw_image(ggleg_dichot_svg, x = -0.025, scale = 1.15) + draw_plot(agnes_dend_clust_dichot_gg, x = -0.15)
ag_dend_dichot_out
ggthemr("flat")

j <- j + 1
```

**Table `r i`.  Analysis #2:  Dichotomous BMI Categories by Cluster, AGNES (*k* = 7)**

```{r agnes_dend_clust_dichot_aggreg, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 6.75, fig.height = 6.75}
ag_dichot_clusts_kbl

i <- i + 1
```

**Figure `r j`.  Analysis #2:  Clustered Radial Dendrogram with Dichotomous BMI Categories, DIANA (*k* = 6)**

```{r diana_dend_clust_dichot, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 8.25, fig.height = 6.75}
ggthemr("flat", spacing = 0.1)
di_dend_dichot_out <- ggdraw() + draw_image(ggleg_dichot_svg, x = -0.025, scale = 1.15) + draw_plot(diana_dend_clust_dichot_gg, x = -0.10)
di_dend_dichot_out
ggthemr("flat")

j <- j + 1
```

**Table `r i`.  Analysis #2:  Dichotomous BMI Categories by Cluster, DIANA (*k* = 6)**

```{r diana_dend_clust_dichot_aggreg, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 6.75, fig.height = 6.75}
di_dichot_clusts_kbl

i <- i + 1
```

### *Analysis #3*

In Analysis #3, AGNES had $k = 10$ clusters (based on the next local maximum in the silhouette plot) and DIANA had $k = 9$ (based on the next elbow in the scree plot).  The AGNES radial dendrogram and corresponding output table are shown in Fig. `r j` and Table `r i` respectively.  The agglomerative approach yielded mean homogeneity 77.5% (range 52.6 - 100.0%), vs. 81.2% (range 57.1 - 100.0%) for DIANA (Fig. `r j + 1` and Table `r i + 1`).  Both approaches resulted in 4 clusters having homogeneity better than 90%.  Increasing the number of clusters resulted in greater homogeneity for both AGNES and DIANA, but the effect was more pronounced for DIANA, which saw an increase of 4.6 percentage points in mean homogeneity above that of Analysis #2, as compared to 1.2 points for AGNES.  Interestingly, the divisive method was able to identify one small cluster (Cluster #4) having majority membership in the "Not Overweight or Obese" category.

**Figure `r j`.  Analysis #3:  Clustered Radial Dendrogram with Dichotomous BMI Categories, AGNES (*k* = 10)**

```{r agnes_dend_clust_dichot_rev, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 8.25, fig.height = 6.75}
ggthemr("flat", spacing = 0.1)
ag_dend_dichot_rev_out <- ggdraw() + draw_image(ggleg_dichot_svg, x = -0.02, scale = 1.15) + draw_plot(agnes_dend_clust_dichot_rev_gg, x = -0.125)
ag_dend_dichot_rev_out
ggthemr("flat")

j <- j + 1
```

**Table `r i`.  Analysis #3:  Dichotomous BMI Categories by Cluster, AGNES (*k* = 10)**

```{r agnes_dend_clust_dichot_rev_aggreg, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 6.75, fig.height = 6.75}
ag_dichot_rev_clusts_kbl

i <- i + 1
```

**Figure `r j`.  Analysis #3:  Clustered Radial Dendrogram with Dichotomous BMI Categories, DIANA (*k* = 9)**

```{r diana_dend_clust_dichot_rev, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 8.25, fig.height = 6.75}
ggthemr("flat", spacing = 0.1)
di_dend_dichot_rev_out <- ggdraw() + draw_image(ggleg_dichot_svg, x = -0.02, scale = 1.15) + draw_plot(diana_dend_clust_dichot_rev_gg, x = -0.10)
di_dend_dichot_rev_out
ggthemr("flat")

j <- j + 1
```

**Table `r i`.  Analysis #3:  Dichotomous BMI Categories by Cluster, DIANA (*k* = 9)**

```{r diana_dend_clust_dichot_rev_aggreg, echo = FALSE, warning = FALSE, paged.print = FALSE, fig.width = 6.75, fig.height = 6.75}
di_dichot_rev_clusts_kbl

i <- i + 1
```

Results for all three clustering analyses are summarized in Table `r i`.  In each case, the divisive method performed better than the agglomerative method using fewer clusters and had a narrower range.  Analysis #3, having dichotomous BMI and a larger number of clusters, yielded the best results for both AGNES and DIANA.


**Table `r i`.  Summary of Homogeneity Results for Clustering Analyses**

| **Analysis #** 	| **AGNES mean (range)** 	| **DIANA mean (range)** 	|
|----------------	|------------------------	|------------------------	|
|        1      	|     52.8% (37.7-83.3%) 	|     54.7% (33.3-77.3%) 	|
|        2      	|    76.3% (52.8-100.0%) 	|     76.6% (57.1-94.9%) 	|
|        3      	|    77.5% (52.6-100.0%) 	|    81.2% (57.1-100.0%) 	|


## **Discussion**

For the supervised approach, the final decision model reached `r paste0(scores_final$pct_acc, "%")` mean accuracy.  While this is an improvement over the initial model, it generally would be considered poor performance for any real-world implementation.  Performance may be improved if the data had fewer missing values; however, much of the NHANES data is collected via survey instruments and missing responses are common with this data collection methodology.  Missing continuous responses were successfully imputed without introducing bias; however, missing categorical responses are more difficult to handle appropriately.

For the unsupervised approach, DIANA outperformed AGNES for all three analyses, indicating that a divisive approach may be more performant with mixed, largely categorical data.  This being an unsupervised analysis, the algorithms are naive to BMI weight category, and the clusters found may represent other similarities discovered among the cases that are not necessarily highly correlated with BMI. However, the high homogeneities found for most clusters in Analysis #3 exceed population prevalence of dichotomous BMI categories and are likely meaningful.  It should be noted that clustering analyses rely on calculation of a dissimilarity matrix, which contains distances between pairwise cases.  With categorical data, particularly nominal, "distance" loses physical meaning, and should not be measured via the typical Euclidean means.  The Gower distance was used to accommodate mixed variables (categorical, ordinal, and nominal), but it's possible an alternative calculation for distance would improve clustering results.

Both the random forest and clustering analyses employed dichotomous BMI categories, grouping together underweight and healthy weight *vs.* overweight and obese.  This approach yielded substantially better results than those found when using the original four levels, but entails the loss of some specificity.  This grouping is defensible, however, because overweight individuals account for greater healthcare expenditure and productivity loss as compared to those of healthy weight, are at risk of becoming obese, and may benefit from similar weight management interventions as would be targeted to those with obesity.

### *Strengths*

Though the variables used in this study were largely categorical, the final clustering analysis performed reasonably well, and could potentially be improved via better variable selection or by using the full dataset rather than a 5% sample.  The 5% sample, however, is also a strength because it allowed for dendrogram visualization, which improves transparency and interpretability of the analysis.

From a technology standpoint, an additional strength of this analysis is that it successfully leveraged a wide variety of libraries in both Python and R in a single environment, and flexibly passed objects between the two domains with little difficulty.  Though not explicitly stated as a study aim, this was a personal learning objective.


### *Limitations*

Because the Scikit-learn `randomForestClassifier()` function was unable to handle missing data, missing values were handled via imputation (continuous variables) or by creating a "missing" category comprising all-cause nonresponses (categorical variables).  Using a "missing" category in this manner may have introduced noise into the dataset, particularly with ordinal variables.  More sophisticated methods of dealing with missing categorical data exist but implementing them was beyond the scope of this study.[31,32]  Future iterations of this analysis should include better handling of missing categorical data and should test other random forest functions $-$ for example, those available in the `xgboost` Python library.  An additional limitation to the random forest approach is that it uses an ensemble of trees and thus no meaningful single decision tree can be visualized, which hampers model transparency.

Another challenge is that variable selection is difficult with categorical variables.  Principal Component Analysis (PCA) is a common method used for choosing variables when considering continuous data, but is not commonly used when data is categorical.  There is, however, evidence in the literature to suggest that categorical variables can be treated via a modified PCA approach, but the packages currently available for this either have limited documentation or are not built on a recent version of R. [33-35]  In the absence of more sophisticated tools, variable selection was based on those characteristics expected to be most closely correlated with weight.


## **Conclusion**

This study indicates that it may be feasible to identify individuals likely to be overweight or obese using demographic, behavioral, and medical variables.  Though refinements are needed, particularly in variable selection and handling missing categorical data, these findings suggest that machine learning methods can be leveraged to appropriately target weight management interventions to an overweight-to-obese population. 


## **References**

1. Selected health conditions and risk factors, by age:  United States, selected years 1988-1994 through 2015-2016.  Centers for Disease Control and Prevention.  Health, United States, 2017:  Trend Tables.  https://www.cdc.gov/nchs/data/hus/2017/053.pdf.  Accessed October 20, 2019.
  
2. Defining Adult Overweight and Obesity.  Centers for Disease Control and Prevention.  https://www.cdc.gov/obesity/adult/defining.html.  Updated April 11, 2017.  Accessed October 22, 2019.

3. Hruby A, Hu FB.  The Epidemiology of Obesity: A Big Picture. *Pharmacoeconomics*. 2015;33(7):673-89.  doi:10.1007/s40273-014-0243-x.

4. Li Q, Blume SW, Huang JC, Hammer M, Ganz ML.  Prevalence and healthcare costs of obesity-related comorbidities:  evidence from an electronic medical records system in the United States.  *J Med Econ*.  2015;18(12):1020-8.  doi:  10.3111/13696998.2015.1067623.

5. National Health and Nutrition Examination Survey.  National Center for Health Statistics.  https://www.cdc.gov/nchs/nhanes/index.htm.  Updated September 24, 2019.  Accessed October 20, 2019.

6. Hales CM, Carroll MD, Fryar CD, Ogden CL.  Prevalence of Obesity Among Adults and Youth:  United States, 2015-2016.  NCHS Data Brief No. 288.  National Center for Health Statistics.  2017.  https://www.cdc.gov/nchs/data/databriefs/db288.pdf.  Accessed October 20, 2019.

7. Ogden CL, Fakhouri TH, Carroll MD, et al. Prevalence of Obesity Among Adults, by Household Income and Education â€“ United States, 2011-2014.  *Morb Mortal Wkly Rep*. 2017;66:1369-1373.  doi:  http://dx.doi.org/10.15585/mmwr.mm6650a1.
  
8. National Health and Nutrition Examination Survey:  Demographic Variables and Sample Weights (DEMO_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT.  Updated September 2017.  Accessed October 27, 2019.

9. National Health and Nutrition Examination Survey:  Questionnaire Data - Diabetes (DIQ_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DIQ_I.XPT.  Updated September 2017.  Accessed October 29, 2019.

10. National Health and Nutrition Examination Survey:  Questionnaire Data - Medical (MCQ_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/MCQ_I.XPT.  Updated September 2017.  Accessed October 29, 2019.

11. National Health and Nutrition Examination Survey:  Examination Data - Blood Pressure (BPX_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BPX_I.XPT.  Updated September 2017.  Accessed October 29, 2019.

12.  Whelton PK, Carey RM, Aronow WS, et al.  2017 ACC/AHA/AAPA/ABC/ACPM/AGS/APhA/ASH/ASPC/NMA/PCNA Guideline for the Prevention, Detection, Evaluation, and Management of High Blood Pressure in Adults:  A Report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines.  *J Am Coll Cardiol*.  2018;71(19):e127-e248.  doi:  10.1016/j.jacc.2017.11.006.

13. Low Blood Pressure.  US National Library of Medicine:  MedlinePlus.  https://medlineplus.gov/ency/article/007278.htm.  Updated November 06, 2019.  Accessed November 10, 2019.

14. National Health and Nutrition Examination Survey:  Questionnaire Data - Physical Activity (PAQ_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/PAQ_I.XPT.  Updated September 2017.  Accessed October 29, 2019.

15. National Health and Nutrition Examination Survey:  Questionnaire Data - Physical Functioning (PFQ_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/PFQ_I.XPT.  Updated September 2017.  Accessed October 29, 2019.

16. National Health and Nutrition Examination Survey:  Questionnaire Data - Mental Health:  Depression Screener (DPQ_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DPQ_I.XPT.  Updated December 2017.  Accessed October 29, 2019.

17. Kroenke K, Spitzer RL. The PHQ-9: A new depression diagnostic and severity measure. *Psychiatric Annals*. 2002;32(9):1-7.  doi:  10.3928/0048-5713-20020901-06.

18. National Health and Nutrition Examination Survey:  Questionnaire Data - Diet Behavior and Nutrition (DBQ_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DBQ_I.XPT.  Updated November 2018.  Accessed October 29, 2019.

19. National Health and Nutrition Examination Survey:  Questionnaire Data - Dietary Interview:  Total Nutrient Intakes, First Day (DR1TOT_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DR1TOT_I.XPT.  Updated July 2018.  Accessed October 29, 2019.

20. National Health and Nutrition Examination Survey:  Examination Data - Body Measures (BMX_I).  National Center for Health Statistics.  https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BMX_I.XPT.  Updated September 2017.  Accessed October 29, 2019.

21. Gower JC. A General Coefficient of Similarity and Some of Its Properties.  *Biometrics*.  1971;27(4):857-71.  doi:  10.2307/2528823.

22. Rousseeuw PJ.  Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.  *Journal of Computational and Applied Mathematics*.  1987;20:53-65.  https://doi.org/10.1016/0377-0427(87)90125-7.

23. sklearn.ensemble.RandomForestClassifier.  Scikit-learn documentation.  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.  Accessed November 10, 2019.

24. sklearn.model_selection.RepeatedKFold.  Scikit-learn documentation.  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html.  Accessed November 10, 2019.

25. sklearn.model_selection.cross_val_score.  Scikit-learn documentation.  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html.  Accessed November 10, 2019.

26. Maechler M, Rousseeuw P, Struyf A, Hubert M, Hornik K.  cluster: Cluster Analysis Basics and Extensions.  R package version 2.1.0.  2019.  https://cran.r-project.org/package=cluster.

27. Hennig C.  fpc:  Flexible Procedures for Clustering.  R package version 2.2-3.  2019.  https://cran.r-project.org/package=fpc.

28. de Vries A, Ripley BD.  ggdendro:  Create Dendrograms and Tree Diagrams Using 'ggplot2'.  R package version 0.1-20.  2016.  https://cran.r-project.org/package=ggdendro.

29. Galili T.  dendextend:  an R package for visualizing, adjusting, and comparing trees of hierarchical clustering.  *Bioinformatics*.  2015;31(22):3718-20.  doi:  10.1093/bioinformatics/btv428.

30. Prioli KM. CSC_8515_Final_Project. https://github.com/kprioliPROF/CSC_8515_Final_Project.

31. Oshungade IO.  Some Methods of Handling Item Non-Response in Categorical Data.  *Journal of the Royal Statistical Society:  Series D (The Statistician)*.  1989;38(4):281-96.
  
32. Copas A, Farewell V.  Dealing with Non-Ignorable Non-Response by Using an "Enthusiasm-To-Respond" Variable. *Journal of the Royal Statistical Society:  Series A (Statistics in Society).*  1998;161(3):385-96.

33. Niitsuma H, Okada T.  Covariance and PCA for Categorical Variables.  Advances in Knowledge Discovery and Data Mining.  Ho TB, Cheung D, Liu H, eds.  *Advances in Knowledge Discovery and Data Mining*.  Berlin:  Springer; 2005.  doi:  10.1007/11430919_61.  arXiv:  0711.4452 [cs.LG].

34. princals:  Categorical principal component analysis (PRINCALS).  Gifi Multivariate Analysis with Optimal Scaling documentation.  https://cran.r-project.org/web/packages/Gifi/index.html.  Updated June 25, 2019.  Accessed November 17, 2019.

35. Chavent M, Kuentz-Simonet V, Labenne A, Saracco J.  Multivariate Analysis of Mixed Data:  The R Package PCAmixdata.  arXiv:1411.4911v4 [stat.CO]  Updated December 8, 2017.  Accessed November 17, 2019.