---
title: "Final Project:  Checkpoint 4"
author: "Katherine M. Prioli"
date: "November 26, 2019"
output:
  pdf_document: default
  html_document: default
geometry: margin = 0.5in
---

## **Progress to Date**

The following progress has been made since Checkpoint 3:

* 
  
## **Next Steps**

* 
  
## **Challenges**





```{python libs, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
# Importing libraries

import math
import numpy as np
import pandas as pd
# import scipy   # Do I need this?  Could use R for stats instead
# import xgboost as xgb

from sklearn import ensemble
from sklearn import tree

from sklearn.metrics import confusion_matrix

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
# from sklearn.model_selection import train_test_split   # I'm not calling this
```

```{python import_subset, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
nhanes_f = r.nhanes_sup_trim_dichot
nhanes_feat_f = nhanes_f.drop(["BMI_cat"], axis = 1)
nhanes_labs_f = nhanes_f.filter(["BMI_cat"])
```

```{python nxk_rkf, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
rfc_f = ensemble.RandomForestClassifier(n_estimators = 200, min_samples_leaf = 25, bootstrap = False) #max_depth = 7, 
rkf_f = RepeatedKFold(n_repeats = 10, n_splits = 3)

scores_f = cross_val_score(rfc_f, nhanes_feat_f, nhanes_labs_f.values.ravel(), cv = rkf_f)
scores_f = pd.DataFrame(scores_f)
```

```{r score_rkf, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
scores_final <- py$scores_f %>% 
  unlist() %>% 
  as_tibble() %>% 
  rename(score = value) %>% 
  select(score) %>% 
  mutate(score = as.numeric(score)) %>% 
  summarize(n_runs = n(),
            pct_acc = round(mean(score), digits = 4) * 100,
            sd_acc = (round(sd(score), digits = 4) * 100),
            min_acc = round(min(score), digits = 4) * 100,
            max_acc = round(max(score), digits = 4) * 100)

scores_final_kbl <- scores_final %>% 
  kable(format = "markdown")
scores_final_kbl
```

## **Unsupervised Approach**

Data was subset to variables with $\ge 90 \%$ non-missing values.  To generate dendrograms for agglomerative clustering (AGNES) and divisive clustering (DIANA), a dissimilarity matrix was calculated based on Gower distance.

```{r dissim, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
dissim_mat <- daisy(nhanes_unsup_trim[ ,2:dim(nhanes_unsup_trim)[2]], metric = "gower")
summary(dissim_mat)
```

Dendrogram for AGNES:

```{r agnes_dend, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
agnes_dend <- agnes(x = (dissim_mat %>% as.matrix()), diss = TRUE, keep.diss = TRUE) %>%
  as.dendrogram() %>%
  ggdendrogram()
agnes_dend
```


Dendrogram for DIANA:

```{r diana_dend, echo = FALSE, paged.print = FALSE, fig.width = 10, fig.height = 6}
diana_dend <- diana(x = (dissim_mat %>% as.matrix()), diss = TRUE, keep.diss = TRUE) %>% 
  as.dendrogram() %>% 
  ggdendrogram()
diana_dend
```

As visual tools, these dendrograms will not be of much use due to the number of cases.

<!-- CONTINUE HERE -->

<!-- Next step is to do stratified sampling (perhaps 5% or 10% of cases), then re-run dissimilarity matrix and dendrograms. -->